<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Centering Image Example</title>
  <style>
      .item {
          text-align: center; /* Center the contents inside the item div */
      }
      .item img {
          display: inline-block; /* Make the image inline-block to apply text-align */
          max-width: 70%; /* Ensure the image scales with the container */
          height: auto; /* Maintain the aspect ratio */
      }
      .subtitle.has-text-centered {
          margin-top: 1rem; /* Add some margin between the image and the subtitle */
      }
  </style>
</head>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=oyfu0pgAAAAJ&hl=zh-CN" target="_blank">Huaxin Zhang</a><sup>1,3</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com.hk/citations?user=3Ifn2DoAAAAJ&hl=zh-CN" target="_blank">Xiaohao Xu</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?user=cQbXvkcAAAAJ&hl=zh-CN" target="_blank">Xiang Wang</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=R5OWszMAAAAJ" target="_blank">Jialong Zuo</a><sup>1</sup>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=2wWnDSQAAAAJ" target="_blank">Chuchu Han</a><sup>1,3</sup></span><br>
                        <span class="author-block">
                          <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=MNKU_WcAAAAJ" target="_blank">Xiaonan Huang</a><sup>2</sup>,</span>
                          <span class="author-block">
                            <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=4tku-lwAAAAJ" target="_blank">Changxin Gao</a><sup>1</sup>,</span>
                            <span class="author-block">
                              <a href="http://faculty.hust.edu.cn/wangyuehuan/zh_CN/index.htm" target="_blank">Yuehuan Wang</a><sup>1</sup>,</span>
                              <span class="author-block">
                                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=ky_ZowEAAAAJ" target="_blank">Nong Sang</a><sup>1&#9993;</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Key Laboratory of Image Processing and Intelligent Control,<br>
                      School of Artificial Intelligence and Automation, Huazhong University of Science and Technology </span>
                      <span class="author-block"><sup>2</sup>University of Michigan, Ann Arbor</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>3</sup>Baidu Inc.</span>
                    <span class="eql-cntrb"><small><br>&#9993;Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2406.12235" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Github link -->
                    <span class="link-block">
                      <a href="https://github.com/pipixin321/HolmesVAD" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>

                  

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/holmesvad.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Examples of Holmes-VAD
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Towards open-ended Video Anomaly Detection (VAD), existing methods often exhibit biased detection when faced with challenging or unseen events and lack interpretability. To address these drawbacks, we propose Holmes-VAD, a novel framework that leverages precise temporal supervision and rich multimodal instructions to enable accurate anomaly localization and comprehensive explanations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Teaser Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser_v2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          In contrast to prevailing VAD approaches <b>(a)</b> that primarily concentrate on identifying anomalies,
          <br>our method <b>(b)</b> facilitates not only <b>unbiased</b> (i.e.,
          less false alarms toward easily cofused or unseen normality) predictions of anomaly scores but also
          <b>explanation</b> of detected anomalies,
          <br> through constructing a large scale VAD dataset with single-frame
          annotations for untrimmed videos and explanable instruction data for trimmed videos.
        </h2>
    </div>
  </div>
</section>
<!-- End teaser Image -->


<!-- Data abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">VAD-Instruct50k</h2>
        <div class="content has-text-justified">
          <p>
          We construct the first largescale multimodal VAD instruction-tuning benchmark, i.e., <b>VAD-Instruct50k</b>. This dataset is created using a carefully designed semi-automatic labeling paradigm. Efficient single-frame annotations are applied to the collected untrimmed videos, which are then synthesized into high-quality analyses of both abnormal and normal video clips using a robust off-the-shelf video captioner and a large language model (LLM).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper data -->

<!-- Data Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/data_engine_v2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Data engine for the proposed VAD-Instruct50k. We collect numerous abnormal/normal
          videos from exsiting datasets, following by a series of annotation enhancement including temporal
          single-frame annotation, event clips generation and event clips captioning. Then we construct the
          instruction data by prompting the powerful LLM with the enhanced annotation. Throughout the
          pipeline, manual work and large fundation models coordinated with each other to ensure efficiency
          and quality in construction.
        </h2>
    </div>
  </div>
</section>
<!-- End data Image -->


<!-- Model abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Holmes-VAD</h2>
        <div class="content has-text-justified">
          <p>
            Building upon the VAD-Instruct50k dataset, we develop a customized solution for interpretable video anomaly detection. We train a lightweight temporal sampler to select frames with high anomaly response and fine-tune a multimodal large language model (LLM) to generate explanatory content.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper model -->

<!-- model Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/framework.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Holmes-VAD takes untrimmed video and user prompt as inputs,
          and takes the anomaly scores and explanation for detected anomalies outputs. The Temporal Sampler
          takes class tokens of frames as input and estimates the anomaly scores, and the dense visual tokens
          are resampled accroding to their anomaly scores before entering the projector.
        </h2>
    </div>
  </div>
</section>
<!-- End model Image -->



<!-- Model abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            In this section, we conduct extensive experiments to thoroughly demonstrate the capabilities of our
            proposed model, i.e., Holmes-VAD.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper model -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/qualitative_results.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Comparisions with Video-LLaVA.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/main_results.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Video Anomaly Detection Performance.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/ablation_results.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Analytic Results.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zhang2024holmes,
        title={Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM},
        author={Zhang, Huaxin and Xu, Xiaohao and Wang, Xiang and Zuo, Jialong and Han, Chuchu and Huang, Xiaonan and Gao, Changxin and Wang, Yuehuan and Sang, Nong},
        journal={arXiv preprint arXiv:2406.12235},
        year={2024}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
